{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Network With Hashing - Hopfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a memory mechanism in a form of a Hopfield network. The stored items are called memory patterns. They are retrieved by a process of the input that is presented to the network dynamics which at some time step reaches a fixed stable point. This means that the input item has been recognized (i.e. there is a memory pattern identical or very similar to it).\n",
    "\n",
    "Even noisy sounds or those corrupted to some extent can be accessed. In other words, if the input is $x_1 + \\delta$ and the stored item is $x_1$, the network will still reach the fixed point of $x_1$ if $\\delta$ is small enough.\n",
    "\n",
    "Additionally, for storage purposes, sounds are transformed each into a hash - with this we reduce their dimensionality. This means we increase the storage capacity. \n",
    "\n",
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the neccessary dependencies.\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with some wav files to test this script.\n",
    "\n",
    "folder_train = \"./wavs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features\n",
    "\n",
    "First, we will transform our .wav files into features, in this case MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mfcc(folder):\n",
    "    \"\"\"\n",
    "    Go through the folder and find all (and only) files ending with .wav\n",
    "    Here, we transform each .wav file into MFCCs and then flatten them into one vector.\n",
    "    We do this because we want one hash per .wav file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : path to folder with wav sounds\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a list of flattened MFCC vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    vectors = []\n",
    "    for file in glob.glob(folder + \"*.wav\", recursive=True):\n",
    "        (rate,sig) = wav.read(file)\n",
    "        mfcc_feat = mfcc(sig,rate)\n",
    "        vect = mfcc_feat.flatten()\n",
    "        vectors.append(vect)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing of features\n",
    "\n",
    "Now we will use these features and transform them into hash vectors, which we will use to store in our memory. We do this to facilitate memory storage: hashes are vectors with reduced dimensionality, with values mostly equal to 0 and a few of them equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_dim(d,k,m,seed):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Define hash parameters.\n",
    "    The hash will be a matrix of the dimension = k*m\n",
    "    We choose a random number k of units of the vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : num\n",
    "        Length of a random vector being stored\n",
    "    k : num\n",
    "        Number of units we randomly choose of the vector\n",
    "    m : num\n",
    "        Number of times we will  do the hashing for some vector\n",
    "    seed : num\n",
    "        We always want the same units randomly chosen\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array \n",
    "        p of dimensions [k,m] represents randomly chosen dimensions\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    assert k <= d\n",
    "    p = np.zeros((m,k,))\n",
    "    np.random.seed(seed)\n",
    "    for i in range(m):\n",
    "        p[i] = np.random.permutation(d)[:k]\n",
    "    return p\n",
    "\n",
    "    \n",
    "def get_hash(vector, k, m, p): \n",
    "    \"\"\"\n",
    "    Transform a vector of speech into a hash\n",
    "    The hash will be a matrix of the dimension = k*m\n",
    "    \n",
    "    Once we have chosen k random dimensions, we look for the highest \n",
    "    value and turn it into 1. Everything else is 0.\n",
    "    We thus get sparse matrices.\n",
    "    We do this m times. Final output is h=k*m.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : np.array\n",
    "        Features (i.e. MFCC) of some sound with dim = 1*n\n",
    "    k : num\n",
    "        Number of units we randomly choose of the vector\n",
    "    m : num\n",
    "        Number of times we will do the hashing for some vector.\n",
    "    p : numpy array\n",
    "        p of dimensions [k,m] represents randomly chosen dimensions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array h of size [1, k*m]\n",
    "    \"\"\"\n",
    "    \n",
    "    h = np.zeros((m,k,))\n",
    "    for i in range(m):\n",
    "        p_line = p[i]\n",
    "        ix = np.argmax(vector[p_line])\n",
    "        hi = np.zeros(k)\n",
    "        hi[ix] = 1\n",
    "        h[i] = hi\n",
    "    h = np.hstack(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test hash:  [1. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "expected_h = np.array([[1,0,0],[0,0,1]]).flatten()\n",
    "vector = np.array([6,4,5,9,2])\n",
    "p0 = hash_dim(len(vector),3,2,2).astype(int)\n",
    "print(\"This is a test hash: \", get_hash(vector, 3, 2, p0))\n",
    "assert get_hash(vector, 3, 2, p0).all() == expected_h.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory storage\n",
    "\n",
    "We must now construct our neuron weight matrix that reopresents the connections between neurons of our memory network.\n",
    "We will first initialize our matrix representing the synaptic weights and then enable subsequent addition of new memories.\n",
    "\n",
    "Synaptic weight matrix is a matrix that represents connections between each and every neuron. Every neuron has a state which can be active or inactive. Initialization of synaptic weights will make the connection between two neurons such that it is strengthened if both neurons are active (and the other way round, it will weaken the connection if one of the neurons is inactive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(N, V=None):\n",
    "    \"\"\"\n",
    "    Eq. (2) from [1]\n",
    "    \n",
    "    Initialize synaptic weights in form of matrix T (symmetric recurrent weight matrix).\n",
    "    This is a memory storage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : num\n",
    "        number of neurons\n",
    "    V : list\n",
    "        list of vectors in a hash form\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    T = np.zeros((N,N))\n",
    "    if V != None:\n",
    "        for vect in V:\n",
    "            #outer_prod = np.outer((2*vect - 1),(2*vect - 1))\n",
    "            T = add_memory(T, vect)\n",
    "            #T += outer_prod\n",
    "        \n",
    "    return T\n",
    "\n",
    "\n",
    "def add_memory(T, new_memory):\n",
    "    \"\"\"\n",
    "    Eq. (2) from [1]\n",
    "    \n",
    "    Update synaptic weights in form of matrix T (symmetric recurrent weight matrix) when adding new memory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    new_memory : numpy array of shape (1,N)\n",
    "        a vector we wish to store\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Renewed memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    outer_prod = np.outer((2*new_memory - 1),(2*new_memory - 1))\n",
    "    T += outer_prod\n",
    "        \n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory retrieval\n",
    "\n",
    "When we want to retrieve a memory, we start with some initial state and then observe the dynamics of the system - if it reaches a stable point, we have accessed to either some stored memory or to a state by default where we can end up if we have not stored something similar to initial state.\n",
    "\n",
    "In other words, we can represent this as a surface with differently sized bumps. We put a ball on this surface and it will roll into the nearest pit, unless we already put it on the already lowest point of the pit.\n",
    "\n",
    "To check whether this lowest point (or stable/fixed point) was reached, we check stability of being there - have we been here a few moments ago? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(T, V):\n",
    "    \"\"\"\n",
    "    Eq. (7) from [1]\n",
    "    \n",
    "    Energy of the system is a monotonically decreasing function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array of shape (N, N)\n",
    "        Memory in form of a matrix (synaptic weights)\n",
    "    V : numpy array\n",
    "        a list of states of activation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num\n",
    "        Energy of the system\n",
    "    \"\"\"\n",
    "    E = 0\n",
    "    for i in range(T.shape[0]):\n",
    "        for j in range(T.shape[1]):\n",
    "            E -= 1/2 * (T[i,j] * V[i] * V[j])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memory(T, V0, U=0, full_trace=True, SEED=27, conv_check_spacing=100):\n",
    "    \"\"\"\n",
    "    Eq. (1) from [1]\n",
    "    \n",
    "    To retrieve a memory, we want to find the stable/fixed point of the \n",
    "    dynamic network represented by matrix T (synaptic weights in which\n",
    "    the memory is stored) when starting from vector V.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T of shape (N, N)\n",
    "        Memory in form of a matrix (synaptic weights)\n",
    "    V0 : a numpy array of shape (1, N)\n",
    "        a vector with which we initialize the network activity (check if it is stored in T)\n",
    "    U : num\n",
    "        a scalar representing the threshold of neuron's state of activity.\n",
    "        Set to 0 by default.\n",
    "        \"With a threshold of 0,the system behaves as a forced categorizer.\" [1]\n",
    "    full_trace : boolean\n",
    "        Set to True by default. This means we will keep all the changes of the initial neuron states\n",
    "        as they change through time.\n",
    "    SEED : num\n",
    "        Used for the random choices of indices, which we can control for replication by always \n",
    "        setting the seed to the same number.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array of shape (1, N)\n",
    "        We return the new / denoised V\n",
    "    \"\"\"\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    V = V0\n",
    "    if full_trace:\n",
    "        V_history = [V.copy()]\n",
    "    \n",
    "    while not has_converged(T, U, V):\n",
    "        for _ in range(conv_check_spacing):\n",
    "            i = random.randrange(V.shape[0])\n",
    "            V[i] = update_neuron(T, U, V, i)\n",
    "            if full_trace:\n",
    "                V_history.append(V.copy())\n",
    "\n",
    "    if full_trace:\n",
    "        return V_history\n",
    "    else:\n",
    "        return V\n",
    "\n",
    "\n",
    "def update_neuron(T, U, V, i):\n",
    "    \"\"\"\n",
    "    We calculate sum_j {T_ji * V[j]}, where T_ij is our synaptic weight between neuron j and i\n",
    "    and V[j] is a j-th neuron state \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    V : num\n",
    "        a scalar representing the neural network state\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num\n",
    "        We return the sum of all components of i-th row of T, each multiplied by V_j\n",
    "    \"\"\"\n",
    "    new_V_i = check_threshold(sum(T[i] * V), U)\n",
    "    return new_V_i\n",
    "\n",
    "\n",
    "\n",
    "def check_threshold(membrane_potential, U):\n",
    "    \"\"\"\n",
    "    Check whether the sum of T_ij * V_j is bigger or smaller than the threshold U\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TV_sum : num\n",
    "        Sum over T_ij * V_j\n",
    "    U : num\n",
    "        a scalar representing a threshold of neuron's state of activity\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num\n",
    "        We return either 0 or 1, depending on TV_sum being smaller\n",
    "        or larger than the threshold U\n",
    "    \"\"\"\n",
    "    if membrane_potential > U:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def has_converged(T, U, V):\n",
    "    \"\"\"\n",
    "    Check whether the new V_i is the same as i-th value of V\n",
    "    and whether the current energy is equal to the previous one\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    V_i : num\n",
    "        state of V's i-th neuron (either active: 1, or inactive: 0)\n",
    "    V : num\n",
    "        a numpy array representing all current neurons' states of activity\n",
    "    i : num\n",
    "        current randomly chosen index\n",
    "    E_list : list\n",
    "        list of all energy values so far\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    boolean\n",
    "        We return False if we satisfy at least one of the conditions,\n",
    "        else we return True\n",
    "    \"\"\"\n",
    "    converged = True\n",
    "    for i, V_i in enumerate(V):\n",
    "        updated_V_i = update_neuron(T, U, V, i)\n",
    "        if updated_V_i != V_i:\n",
    "            converged = False\n",
    "            break\n",
    "    return converged    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy T matrix: \n",
      " [[ 2. -2.]\n",
      " [-2.  2.]]\n",
      "Retrieved memory of [0,0] - unstored:\n",
      " [array([0, 0])]\n",
      "Retrieved memory of [1,1] - unstored:\n",
      " [array([1, 1]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0])]\n",
      "Retrieved memory of [0,1] - stored:\n",
      " [array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "toy_V1 = np.array([0,1])\n",
    "toy_V2 = np.array([1,0])\n",
    "toy_T = initialize_network(2, [toy_V1, toy_V2])\n",
    "\n",
    "print(\"Toy T matrix: \\n\", toy_T)\n",
    "print(\"Retrieved memory of [0,0] - unstored:\\n\", retrieve_memory(toy_T, np.array([0,0])))\n",
    "print(\"Retrieved memory of [1,1] - unstored:\\n\", retrieve_memory(toy_T, np.array([1,1])))\n",
    "print(\"Retrieved memory of [0,1] - stored:\\n\", retrieve_memory(toy_T, toy_V1))\n",
    "\n",
    "# print(retrieve_memory(T, np.array([1,1,1,1,1,1,0,0,0,0,0,0,0,0,0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n",
    "\n",
    "We can now inspect our memory network and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with random dimensions k, m, N, which determine the size of T and of hash vectors:\n",
    "\n",
    "k = 5\n",
    "m = 3\n",
    "N = 15\n",
    "V =[]\n",
    "\n",
    "p = hash_dim(325,k,m,27).astype(int)\n",
    "mfccs_vectors = make_mfcc(folder_train)\n",
    "for vect in mfccs_vectors:\n",
    "    v = get_hash(vect, k, m, p)\n",
    "    V.append(v)\n",
    "\n",
    "T = initialize_network(N, V[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect how one of our hashed vectors looks like:\n",
    "\n",
    "print(V[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see here how our matrix T (symmetric recurrent weight matrix) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.  74.  90. ... 100. 100.  88.]\n",
      " [ 74. 100.  64. ...  74.  74.  62.]\n",
      " [ 90.  64. 100. ...  90.  90.  78.]\n",
      " ...\n",
      " [100.  74.  90. ... 100. 100.  88.]\n",
      " [100.  74.  90. ... 100. 100.  88.]\n",
      " [ 88.  62.  78. ...  88.  88. 100.]]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect how our transition matrix looks like:\n",
    "\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our memory and see what the energy function of a vector already stored in memory is and compare it to a vector not stored in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory retrieval changes of V_unstored - unstored:\n",
      " 301\n",
      "Memory retrieval changes of V[1] - stored:\n",
      " 1\n",
      "Retrieved memory of V[1] - stored:\n",
      " [array([1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 1., 1.])]\n",
      "Energy: -128762.0\n"
     ]
    }
   ],
   "source": [
    "# Let us now see how the energy function works when we tested it:\n",
    "# E_test is a vector impossible to obtain from our hashing method - \n",
    "# it should show no fixed point\n",
    "# E_V0 uses as test the first hashed vector (identical to the stored data)\n",
    "\n",
    "V_unstored = np.array([3, 1, 4, 1, 0, 0, 0, 0, 7, 0, 9, 0, 7, 0, 4])\n",
    "print(\"Memory retrieval changes of V_unstored - unstored:\\n\", len(retrieve_memory(T, V_unstored)))\n",
    "\n",
    "print(\"Memory retrieval changes of V[1] - stored:\\n\", len(retrieve_memory(T, V[1])))\n",
    "print(\"Retrieved memory of V[1] - stored:\\n\", retrieve_memory(T, V[1]))\n",
    "\n",
    "E = energy(T, V_unstored)\n",
    "\n",
    "# We can see the energy as it gradually stabilizes\n",
    "print(\"Energy:\", E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the energy data\n",
    "\n",
    "plt.plot(E)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558. \n",
    "[https://doi.org/10.1073/pnas.79.8.2554]\n",
    "\n",
    "[2] Andoni, A., & Indyk, P. (2006, October). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS'06) (pp. 459-468). IEEE.\n",
    "[https://10.1109/FOCS.2006.49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# robustness\n",
    "# precision/recall\n",
    "# size of input that can be stocked\n",
    "# nature of stimuli\n",
    "# nature of hash\n",
    "# put silence at the end so all have the same length\n",
    "# link to ref - this equation num. from this paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
