{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Network With Hashing - Hopfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a memory mechanism in a form of a Hopfield network. The stored items are called memory patterns. They are retrieved by a process of the input that is presented to the network dynamics which at some time step reaches a fixed stable point. This means that the input item has been recognized (i.e. there is a memory pattern identical or very similar to it).\n",
    "\n",
    "Even noisy sounds or those corrupted to some extent can be accessed. In other words, if the input is $x_1 + \\delta$ and the stored item is $x_1$, the network will still reach the fixed point of $x_1$ if $\\delta$ is small enough.\n",
    "\n",
    "Additionally, for storage purposes, sounds are transformed each into a hash - with this we reduce their dimensionality. This means we increase the storage capacity. \n",
    "\n",
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the neccessary dependencies.\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with some wav files to test this script.\n",
    "\n",
    "folder_train = \"./waveforms/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features\n",
    "\n",
    "First, we will transform our .wav files into features, in this case MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mfcc(folder):\n",
    "    \"\"\"\n",
    "    Go through the folder and find all (and only) files ending with .wav\n",
    "    Here, we transform each .wav file into MFCCs and then flatten them into one vector.\n",
    "    We do this because we want one hash per .wav file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : path to folder with wav sounds\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a list of flattened MFCC vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    vectors = []\n",
    "    for file in glob.glob(folder + \"*.wav\", recursive=True):\n",
    "        (rate,sig) = wav.read(file)\n",
    "        mfcc_feat = mfcc(sig,rate)\n",
    "        vect = mfcc_feat.flatten()\n",
    "        vectors.append(vect)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing of features\n",
    "\n",
    "Now we will use these features and transform them into hash vectors, which we will use to store in our memory. We do this to facilitate memory storage: hashes are vectors with reduced dimensionality, with values mostly equal to 0 and a few of them equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test hash:  [1. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "def hash_dim(d,k,m,seed):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Define hash parameters.\n",
    "    The hash will be a matrix of the dimension = k*m\n",
    "    We choose a random number k of units of the vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : num\n",
    "        Length of a random vector being stored\n",
    "    k : num\n",
    "        Number of units we randomly choose of the vector\n",
    "    m : num\n",
    "        Number of times we will  do the hashing for some vector\n",
    "    seed : num\n",
    "        We always want the same units randomly chosen\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array \n",
    "        p of dimensions [k,m] represents randomly chosen dimensions\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    assert k <= d\n",
    "    p = np.zeros((m,k,))\n",
    "    np.random.seed(seed)\n",
    "    for i in range(m):\n",
    "        p[i] = np.random.permutation(d)[:k]\n",
    "    return p\n",
    "\n",
    "    \n",
    "def get_hash(vector, k, m, p): \n",
    "    \"\"\"\n",
    "    Transform a vector of speech into a hash\n",
    "    The hash will be a matrix of the dimension = k*m\n",
    "    \n",
    "    Once we have chosen k random dimensions, we look for the highest \n",
    "    value and turn it into 1. Everything else is 0.\n",
    "    We thus get sparse matrices.\n",
    "    We do this m times. Final output is h=k*m.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : np.array\n",
    "        Features (i.e. MFCC) of some sound with dim = 1*n\n",
    "    k : num\n",
    "        Number of units we randomly choose of the vector\n",
    "    m : num\n",
    "        Number of times we will do the hashing for some vector.\n",
    "    p : numpy array\n",
    "        p of dimensions [k,m] represents randomly chosen dimensions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array h of size [1, k*m]\n",
    "    \"\"\"\n",
    "    \n",
    "    h = np.zeros((m,k,))\n",
    "    for i in range(m):\n",
    "        p_line = p[i]\n",
    "        ix = np.argmax(vector[p_line])\n",
    "        hi = np.zeros(k)\n",
    "        hi[ix] = 1\n",
    "        h[i] = hi\n",
    "    h = np.hstack(h)\n",
    "    return h\n",
    "\n",
    "\n",
    "# TEST\n",
    "\n",
    "expected_h = np.array([[1,0,0],[0,0,1]]).flatten()\n",
    "vector = np.array([6,4,5,9,2])\n",
    "p0 = hash_dim(len(vector),3,2,2).astype(int)\n",
    "print(\"This is a test hash: \", get_hash(vector, 3, 2, p0))\n",
    "assert get_hash(vector, 3, 2, p).all() == expected_h.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory storage\n",
    "\n",
    "We must now construct our neuron weight matrix that reopresents the connections between neurons of our memory network.\n",
    "We will first initialize our matrix representing the synaptic weights and then enable subsequent addition of new memories.\n",
    "\n",
    "Synaptic weight matrix is a matrix that represents connections between each and every neuron. Every neuron has a state which can be active or inactive. Initialization of synaptic weights will make the connection between two neurons such that it is strengthened if both neurons are active (and the other way round, it will weaken the connection if one of the neurons is inactive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(N, V):\n",
    "    \"\"\"\n",
    "    Eq. [2] from [1]\n",
    "    \n",
    "    Initialize synaptic weights in form of matrix T (symmetric recurrent weight matrix).\n",
    "    This is a memory storage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : num\n",
    "        number of neurons\n",
    "    V : list\n",
    "        list of vectors in a hash form\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    T = np.zeros((N,N))\n",
    "    for vect in V:\n",
    "        outer_prod = np.outer((2*vect - 1),(2*vect - 1))\n",
    "        T += outer_prod\n",
    "        \n",
    "    return T\n",
    "\n",
    "\n",
    "def add_memory(T, new_memory):\n",
    "    \"\"\"\n",
    "    Eq. [2] from [1]\n",
    "    \n",
    "    Initialize synaptic weights in form of matrix T (symmetric recurrent weight matrix).\n",
    "    This is a memory storage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    new_memory : numpy array of shape (1,N)\n",
    "        a vector we wish to store\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Renewed memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    outer_prod = np.outer((2*new_memory - 1),(2*new_memory - 1))\n",
    "    T += outer_prod\n",
    "        \n",
    "    return T\n",
    "\n",
    "# TEST\n",
    "# calcul a la main d'une matrice + added memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory retrieval\n",
    "\n",
    "To see when we have reached a stable fixed point, we use the energy function E, which is a monotonically decreasing function - state changes continue until a local E is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def energy(T, V):\n",
    "    \"\"\"\n",
    "    Eq. [7] from [1]\n",
    "    \n",
    "    Energy of the system is a monotonically decreasing function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    V : numpy array\n",
    "        a set of vectors we have stored\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a list\n",
    "        Energy of the system\n",
    "    \"\"\"\n",
    "    E_t1 = 0\n",
    "    E = []\n",
    "    for i in range(T.shape[0]):\n",
    "        for j in range(len(V)):\n",
    "            for k in range(len(V)):\n",
    "                E_t = E_t1\n",
    "                E_t1 -= 1/2 * (T[i,j] * V[j][k] * V[k][j])\n",
    "                E.append(E_t1)\n",
    "        return E\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# take initalized T & some vect - uniformly take with replacement randomly a neiron from this vector\n",
    "# if sum > Ui but does not change neuron state - point fixe - check de temps en temps\n",
    "# update asynchrone acc to [1]\n",
    "# look at E at every step\n",
    "# when stabilizes\n",
    "\n",
    "def asynchronous_update(T, V, U=0):\n",
    "    \"\"\"\n",
    "    Eq. [1] from [1]\n",
    "    \n",
    "    To retrieve a memory, we want to find the stable/fixed point.\n",
    "    This is achieved by comparing a new vector V to the matrix T,\n",
    "    which represents synaptic weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    V : numpy array of shape (1,N)\n",
    "        a vector we wish to store\n",
    "    U : num\n",
    "        a scalar representing a threshold of neuron's state of activity\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    T_sum = 0\n",
    "    for k in range(len(V)):\n",
    "        for i in range(T.shape[0]):\n",
    "            for j in range(T.shape[0]):\n",
    "                T_sum += T[i][j] * V[k] \n",
    "                \n",
    "    return Vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect our memory network and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (800) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (800) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    }
   ],
   "source": [
    "# Test with random dimensions k, m, N, which determine the size of T and of hash vectors:\n",
    "\n",
    "k = 5\n",
    "m = 3\n",
    "N = 15\n",
    "V =[]\n",
    "\n",
    "p = hash_dim(325,k,m,27).astype(int)\n",
    "mfccs_vectors = make_mfcc(folder_train)\n",
    "for vect in mfccs_vectors:\n",
    "    v = get_hash(vect, k, m, p)\n",
    "    V.append(v)\n",
    "\n",
    "T = initialize_network(N, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important assumption here is that Hopfield uses $V_i$ as a state of neuron activation which can have a value 0 (\"not firing\") or 1 (\"maximally firing\"). We also obtained 1s and 0s for hashing purposes (you can see what a vector will look like in a cell below). I am therefore here making an assumption that values in the hash vector equate whether the neuron is active or not, since I do not know how else to obtain the neuron activation (this assumption may be wrong). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), array([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect how one of our hashed vectors looks like:\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see here how our matrix T (symmetric recurrent weight matrix) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.  3.  3.  3.  3.  7.  7.  3. -3.  7.  7.  7.  5. -5.  7.]\n",
      " [ 3.  9.  1.  1.  1.  5.  5.  1. -1.  5.  5.  5.  3. -3.  5.]\n",
      " [ 3.  1.  9.  1.  1.  5.  5.  9. -9.  5.  5.  5.  3. -3.  5.]\n",
      " [ 3.  1.  1.  9.  1.  5.  5.  1. -1.  5.  5.  5.  7. -7.  5.]\n",
      " [ 3.  1.  1.  1.  9.  5.  5.  1. -1.  5.  5.  5.  3. -3.  5.]\n",
      " [ 7.  5.  5.  5.  5.  9.  9.  5. -5.  9.  9.  9.  7. -7.  9.]\n",
      " [ 7.  5.  5.  5.  5.  9.  9.  5. -5.  9.  9.  9.  7. -7.  9.]\n",
      " [ 3.  1.  9.  1.  1.  5.  5.  9. -9.  5.  5.  5.  3. -3.  5.]\n",
      " [-3. -1. -9. -1. -1. -5. -5. -9.  9. -5. -5. -5. -3.  3. -5.]\n",
      " [ 7.  5.  5.  5.  5.  9.  9.  5. -5.  9.  9.  9.  7. -7.  9.]\n",
      " [ 7.  5.  5.  5.  5.  9.  9.  5. -5.  9.  9.  9.  7. -7.  9.]\n",
      " [ 7.  5.  5.  5.  5.  9.  9.  5. -5.  9.  9.  9.  7. -7.  9.]\n",
      " [ 5.  3.  3.  7.  3.  7.  7.  3. -3.  7.  7.  7.  9. -9.  7.]\n",
      " [-5. -3. -3. -7. -3. -7. -7. -3.  3. -7. -7. -7. -9.  9. -7.]\n",
      " [ 7.  5.  5.  5.  5.  9.  9.  5. -5.  9.  9.  9.  7. -7.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect how our transition matrix looks like:\n",
    "\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our memory and see what the energy function of a vector already stored in memory is and compare it to a vector not stored in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -4.5, -4.5, -4.5, -4.5, -4.5, -6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -6.0, -7.5, -7.5, -7.5, -7.5, -7.5, -7.5, -7.5, -7.5, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -10.5, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -9.0, -7.5]\n"
     ]
    }
   ],
   "source": [
    "# Let us now see how the energy function works when we tested it:\n",
    "# E_test is a vector impossible to obtain from our hashing method - \n",
    "# it should show no fixed point\n",
    "# E_V0 uses as test the first hashed vector (identical to the stored data)\n",
    "\n",
    "V_unstored = [3, 1, 4, 1, 0, 0, 0, 0, 7, 0, 9, 0, 7, 0, 4]\n",
    "E = energy(T, V)\n",
    "\n",
    "# We can see the energy as it gradually stabilizes\n",
    "print(energy(T, V,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the energy data\n",
    "\n",
    "# plt.plot(E)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# robustness\n",
    "# precision/recall\n",
    "# size of input that can be stocked\n",
    "# nature of stimuli\n",
    "# nature of hash\n",
    "# put silence at the end so all have the same length\n",
    "# link to ref - this equation num. from this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558. \n",
    "\n",
    "[2] Andoni, A., & Indyk, P. (2006, October). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS'06) (pp. 459-468). IEEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
