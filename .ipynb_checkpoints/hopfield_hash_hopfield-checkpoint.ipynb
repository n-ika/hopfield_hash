{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Network With Hashing - Hopfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a memory mechanism in a form of a Hopfield network. The stored items are called memory patterns. They are retrieved by a process of the input that is presented to the network dynamics which at some time step reaches a fixed stable point. This means that the input item has been recognized (i.e. there is a memory pattern identical or very similar to it).\n",
    "\n",
    "Even noisy sounds or those corrupted to some extent can be accessed. In other words, if the input is $x_1 + \\delta$ and the stored item is $x_1$, the network will still reach the fixed point of $x_1$ if $\\delta$ is small enough.\n",
    "\n",
    "Additionally, for storage purposes, sounds are transformed each into a hash - with this we reduce their dimensionality. This means we increase the storage capacity. \n",
    "\n",
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the neccessary dependencies.\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with some wav files to test this script.\n",
    "\n",
    "folder_train = \"./wavs/\"\n",
    "\n",
    "test_folder = \"./test_wavs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features\n",
    "\n",
    "First, we will transform our .wav files into features, in this case MFCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mfcc(folder):\n",
    "    \"\"\"\n",
    "    Go through the folder and find all (and only) files ending with .wav\n",
    "    Here, we transform each .wav file into MFCCs and then flatten them into one vector.\n",
    "    We do this because we want one hash per .wav file.\n",
    "    \n",
    "    Any file shorter than the longest file in the folder will be padded with values 0,\n",
    "    so that all concatenated file vectors are of the same length.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : path to folder with wav sounds\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a list of flattened MFCC vectors\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for file in glob.glob(folder + \"*.wav\", recursive=True):\n",
    "        y, sr = librosa.load(file)\n",
    "        mfcc_feat = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        vect = mfcc_feat.flatten()\n",
    "        vectors.append(vect)\n",
    "    # find the largest vector\n",
    "    max_length = len(max(vectors, key=lambda p: len(p)))\n",
    "    # append zeros to all the other vectors\n",
    "    for i in range(len(vectors)):\n",
    "        vectors[i] = np.pad(vect, (0,max_length-len(vect)))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing of features\n",
    "\n",
    "Now we will use these features and transform them into hash vectors, which we will use to store in our memory. We do this to facilitate memory storage: hashes are vectors with reduced dimensionality, with values mostly equal to 0 and a few of them equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_dim(d,k,m,seed):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Define hash parameters.\n",
    "    The hash will be a matrix of the dimension = k*m\n",
    "    We choose a random number k of units of the vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : num\n",
    "        Length of a random vector being stored\n",
    "    k : num\n",
    "        Number of units we randomly choose of the vector\n",
    "    m : num\n",
    "        Number of times we will  do the hashing for some vector\n",
    "    seed : num\n",
    "        We always want the same units randomly chosen\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array \n",
    "        p of dimensions [k,m] represents randomly chosen dimensions\n",
    "    \n",
    "    \"\"\"   \n",
    "    assert k <= d\n",
    "    p = np.zeros((m,k,))\n",
    "    np.random.seed(seed)\n",
    "    for i in range(m):\n",
    "        p[i] = np.random.permutation(d)[:k]\n",
    "    return p\n",
    "\n",
    "    \n",
    "def get_hash(vector, k, m, p): \n",
    "    \"\"\"\n",
    "    Transform a vector of speech into a hash\n",
    "    The hash will be a matrix of the dimension = k*m\n",
    "    \n",
    "    Once we have chosen k random dimensions, we look for the highest \n",
    "    value and turn it into 1. Everything else is 0.\n",
    "    We thus get sparse matrices.\n",
    "    We do this m times. Final output is h=k*m.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : np.array\n",
    "        Features (i.e. MFCC) of some sound with dim = 1*n\n",
    "    k : num\n",
    "        Number of units we randomly choose of the vector\n",
    "    m : num\n",
    "        Number of times we will do the hashing for some vector.\n",
    "    p : numpy array\n",
    "        p of dimensions [k,m] represents randomly chosen dimensions\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array h of size [1, k*m]\n",
    "    \"\"\"\n",
    "    h = np.zeros((m,k,))\n",
    "    for i in range(m):\n",
    "        p_line = p[i]\n",
    "        ix = np.argmax(vector[p_line])\n",
    "        hi = np.zeros(k)\n",
    "        hi[ix] = 1\n",
    "        h[i] = hi\n",
    "    h = np.hstack(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 µs ± 512 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "This is a test hash:  [1. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "expected_h = np.array([[1,0,0],[0,0,1]]).flatten()\n",
    "vector = np.array([6,4,5,9,2])\n",
    "# %timeit hash_dim(len(vector),3,2,2).astype(int)\n",
    "p0 = hash_dim(len(vector),3,2,2).astype(int)\n",
    "print(\"This is a test hash: \", get_hash(vector, 3, 2, p0))\n",
    "assert get_hash(vector, 3, 2, p0).all() == expected_h.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory storage\n",
    "\n",
    "We must now construct our neuron weight matrix that reopresents the connections between neurons of our memory network.\n",
    "We will first initialize our matrix representing the synaptic weights and then enable subsequent addition of new memories.\n",
    "\n",
    "Synaptic weight matrix is a matrix that represents connections between each and every neuron. Every neuron has a state which can be active or inactive. Initialization of synaptic weights will make the connection between two neurons such that it is strengthened if both neurons are active (and the other way round, it will weaken the connection if one of the neurons is inactive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(N, V=None):\n",
    "    \"\"\"\n",
    "    Eq. (2) from [1]\n",
    "    \n",
    "    Initialize synaptic weights in form of matrix T (symmetric recurrent weight matrix).\n",
    "    This is a memory storage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : num\n",
    "        number of neurons\n",
    "    V : list\n",
    "        list of vectors in a hash form\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    %timeit\n",
    "    T = np.zeros((N,N))\n",
    "    if V != None:\n",
    "        for vect in V:\n",
    "            #outer_prod = np.outer((2*vect - 1),(2*vect - 1))\n",
    "            T = add_memory(T, vect)\n",
    "            #T += outer_prod\n",
    "        \n",
    "    return T\n",
    "\n",
    "\n",
    "def add_memory(T, new_memory):\n",
    "    \"\"\"\n",
    "    Eq. (2) from [1]\n",
    "    \n",
    "    Update synaptic weights in form of matrix T (symmetric recurrent weight matrix) when adding new memory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    new_memory : numpy array of shape (1,N)\n",
    "        a vector we wish to store\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array T of shape (N, N)\n",
    "        Renewed memory storage in form of a matrix (synaptic weights)\n",
    "        Its dimensions are determined by N=k*m (hash parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    outer_prod = np.outer((2*new_memory - 1),(2*new_memory - 1))\n",
    "    T += outer_prod\n",
    "        \n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory retrieval\n",
    "\n",
    "When we want to retrieve a memory, we start with some initial state and then observe the dynamics of the system - if it reaches a stable point, we have accessed to either some stored memory or to a state by default where we can end up if we have not stored something similar to initial state.\n",
    "\n",
    "In other words, we can represent this as a surface with differently sized bumps. We put a ball on this surface and it will roll into the nearest pit, unless we already put it on the already lowest point of the pit.\n",
    "\n",
    "To check whether this lowest point (or stable/fixed point) was reached, we check stability of being there - have we been here a few moments ago? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(T, V):\n",
    "    \"\"\"\n",
    "    Eq. (7) from [1]\n",
    "    \n",
    "    Energy of the system is a monotonically decreasing function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array of shape (N, N)\n",
    "        Memory in form of a matrix (synaptic weights)\n",
    "    V : numpy array\n",
    "        a list of states of activation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num\n",
    "        Energy of the system\n",
    "    \"\"\"\n",
    "    E = 0\n",
    "    for i in range(T.shape[0]):\n",
    "        for j in range(T.shape[1]):\n",
    "            E -= 1/2 * (T[i,j] * V[i] * V[j])\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memory(T, V0, U=0, full_trace=True, SEED=27, conv_check_spacing=100):\n",
    "    \"\"\"\n",
    "    Eq. (1) from [1]\n",
    "    \n",
    "    To retrieve a memory, we want to find the stable/fixed point of the \n",
    "    dynamic network represented by matrix T (synaptic weights in which\n",
    "    the memory is stored) when starting from vector V.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T of shape (N, N)\n",
    "        Memory in form of a matrix (synaptic weights)\n",
    "    V0 : a numpy array of shape (1, N)\n",
    "        a vector with which we initialize the network activity (check if it is stored in T)\n",
    "    U : num\n",
    "        a scalar representing the threshold of neuron's state of activity.\n",
    "        Set to 0 by default.\n",
    "        \"With a threshold of 0,the system behaves as a forced categorizer.\" [1]\n",
    "    full_trace : boolean\n",
    "        Set to True by default. This means we will keep all the changes of the initial neuron states\n",
    "        as they change through time.\n",
    "    SEED : num\n",
    "        Used for the random choices of indices, which we can control for replication by always \n",
    "        setting the seed to the same number.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a numpy array of shape (1, N)\n",
    "        We return the new / denoised V\n",
    "    \"\"\"\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    V = V0\n",
    "    if full_trace:\n",
    "        V_history = [V.copy()]\n",
    "    \n",
    "    while not has_converged(T, U, V):\n",
    "        for _ in range(conv_check_spacing):\n",
    "            i = random.randrange(V.shape[0])\n",
    "            V[i] = update_neuron(T, U, V, i)\n",
    "            if full_trace:\n",
    "                V_history.append(V.copy())\n",
    "\n",
    "    if full_trace:\n",
    "        return V_history\n",
    "    else:\n",
    "        return V\n",
    "\n",
    "\n",
    "def update_neuron(T, U, V, i):\n",
    "    \"\"\"\n",
    "    We calculate sum_j {T_ji * V[j]}, where T_ij is our synaptic weight between neuron j and i\n",
    "    and V[j] is a j-th neuron state \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    T : a numpy array T_sum of shape (N, N)\n",
    "        Initialized memory storage in form of a matrix (synaptic weights)\n",
    "    V : num\n",
    "        a scalar representing the neural network state\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num\n",
    "        We return the sum of all components of i-th row of T, each multiplied by V_j\n",
    "    \"\"\"\n",
    "    new_V_i = check_threshold(sum(T[i] * V), U)\n",
    "    return new_V_i\n",
    "\n",
    "\n",
    "\n",
    "def check_threshold(membrane_potential, U):\n",
    "    \"\"\"\n",
    "    Check whether the sum of T_ij * V_j is bigger or smaller than the threshold U\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TV_sum : num\n",
    "        Sum over T_ij * V_j\n",
    "    U : num\n",
    "        a scalar representing a threshold of neuron's state of activity\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num\n",
    "        We return either 0 or 1, depending on TV_sum being smaller\n",
    "        or larger than the threshold U\n",
    "    \"\"\"\n",
    "    if membrane_potential > U:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def has_converged(T, U, V):\n",
    "    \"\"\"\n",
    "    Check whether the new V_i is the same as i-th value of V\n",
    "    and whether the current energy is equal to the previous one\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    V_i : num\n",
    "        state of V's i-th neuron (either active: 1, or inactive: 0)\n",
    "    V : num\n",
    "        a numpy array representing all current neurons' states of activity\n",
    "    i : num\n",
    "        current randomly chosen index\n",
    "    E_list : list\n",
    "        list of all energy values so far\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    boolean\n",
    "        We return False if we satisfy at least one of the conditions,\n",
    "        else we return True\n",
    "    \"\"\"\n",
    "    converged = True\n",
    "    for i, V_i in enumerate(V):\n",
    "        updated_V_i = update_neuron(T, U, V, i)\n",
    "        if updated_V_i != V_i:\n",
    "            converged = False\n",
    "            break\n",
    "    return converged    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.3 µs ± 2.73 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "Toy T matrix: \n",
      " [[ 2. -2.]\n",
      " [-2.  2.]]\n",
      "Retrieved memory of [0,0] - unstored:\n",
      " [array([0, 0])]\n",
      "Retrieved memory of [1,1] - unstored:\n",
      " [array([1, 1]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0])]\n",
      "Retrieved memory of [9,9] - unstored:\n",
      " [array([9, 9]), array([9, 0]), array([9, 0]), array([9, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0]), array([1, 0])]\n",
      "Retrieved memory of [0,1] - stored:\n",
      " [array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "toy_V1 = np.array([0,1])\n",
    "toy_V2 = np.array([1,0])\n",
    "%timeit initialize_network(2, [toy_V1, toy_V2])\n",
    "toy_T = initialize_network(2, [toy_V1, toy_V2])\n",
    "print(\"Toy T matrix: \\n\", toy_T)\n",
    "print(\"Retrieved memory of [0,0] - unstored:\\n\", retrieve_memory(toy_T, np.array([0,0])))\n",
    "print(\"Retrieved memory of [1,1] - unstored:\\n\", retrieve_memory(toy_T, np.array([1,1])))\n",
    "print(\"Retrieved memory of [9,9] - unstored:\\n\", retrieve_memory(toy_T, np.array([9,9])))\n",
    "print(\"Retrieved memory of [0,1] - stored:\\n\", retrieve_memory(toy_T, toy_V1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing \n",
    "\n",
    "We can now inspect our memory network and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with random dimensions k, m, N, which determine the size of T and of hash vectors:\n",
    "\n",
    "k = 5\n",
    "m = 3\n",
    "N = 15\n",
    "V =[]\n",
    "\n",
    "# %timeit hash_dim(N,k,m,27).astype(int)\n",
    "p = hash_dim(N,k,m,27).astype(int)\n",
    "\n",
    "mfccs_vectors = make_mfcc(test_folder)\n",
    "# # load the mffccs\n",
    "# with open(\"test.txt\", \"rb\") as fp:\n",
    "#     mfccs_vectors = pickle.load(fp)\n",
    "    \n",
    "for vect in mfccs_vectors:\n",
    "    v = get_hash(vect, k, m, p)\n",
    "    V.append(v)\n",
    "\n",
    "T = initialize_network(N, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect how one of our hashed vectors looks like:\n",
    "\n",
    "print(V[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see here how our matrix T (symmetric recurrent weight matrix) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [-4. -4. -4.  4. -4. -4. -4. -4.  4. -4. -4. -4. -4. -4.  4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [-4. -4. -4.  4. -4. -4. -4. -4.  4. -4. -4. -4. -4. -4.  4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [ 4.  4.  4. -4.  4.  4.  4.  4. -4.  4.  4.  4.  4.  4. -4.]\n",
      " [-4. -4. -4.  4. -4. -4. -4. -4.  4. -4. -4. -4. -4. -4.  4.]]\n"
     ]
    }
   ],
   "source": [
    "# We can inspect how our transition matrix looks like:\n",
    "\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our memory with the stored and unstored data. We want to see whether the system finds the stored memories and whether it finds the general fixed point (only 0s) for the unstored ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "[1 1 1 0 1 1 1 1 0 1 1 1 1 1 0]\n",
      "[1 1 1 0 1 1 1 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "V_unstored_1 = np.array([3, 1, 4, 1, 0, 0, 0, 0, 7, 0, 9, 0, 7, 0, 4])\n",
    "V_unstored_2 = np.array([1,1,1,1,1,1,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "print(retrieve_memory(T, V[1])[-1])\n",
    "print(retrieve_memory(T, V_unstored_1)[-1])\n",
    "print(retrieve_memory(T, V_unstored_2)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision-recall score for the unstored: 1.00\n",
      "Precision-recall score for the stored: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_unstored = average_precision_score(retrieve_memory(T, V_unstored_1)[-1], V_unstored_1)\n",
    "average_precision_stored = average_precision_score(retrieve_memory(T, V[1])[-1], V[1])\n",
    "\n",
    "print('Precision-recall score for the unstored: {0:0.2f}'.format(\n",
    "      average_precision_unstored))\n",
    "print('Precision-recall score for the stored: {0:0.2f}'.format(\n",
    "      average_precision_stored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558. \n",
    "[https://doi.org/10.1073/pnas.79.8.2554]\n",
    "\n",
    "[2] Andoni, A., & Indyk, P. (2006, October). Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS'06) (pp. 459-468). IEEE.\n",
    "[https://10.1109/FOCS.2006.49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# robustness\n",
    "# precision/recall\n",
    "# size of input that can be stocked\n",
    "# nature of stimuli\n",
    "# nature of hash\n",
    "# put silence at the end so all have the same length\n",
    "# link to ref - this equation num. from this paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
